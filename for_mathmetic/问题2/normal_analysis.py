import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
# æ–°å¢ï¼šç”¨äºæ„é€ åŸºäºè„šæœ¬ç›®å½•çš„ç»å¯¹è·¯å¾„
import os
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

print("åŸºäºå›ºå®šBMIåŒºé—´çš„åˆ†ç»„åˆ†æ")

# æ•°æ®åŠ è½½
script_dir = os.path.dirname(os.path.abspath(__file__))
boys_csv_path = os.path.join(script_dir, "clean_data_csv", "boys_clean_2.csv")
boys_data = pd.read_csv(boys_csv_path)
print(f"æ•°æ®: {len(boys_data)} æ¡è®°å½•")

# åŸºäºå›ºå®šBMIåŒºé—´åˆ†ç»„
def fixed_bmi_grouping_analysis(df):
    """åŸºäºå›ºå®šBMIåŒºé—´è¿›è¡Œåˆ†ç»„å’Œé£é™©åˆ†æ"""
    
    # å®šä¹‰BMIåŒºé—´
    bins = [0, 28, 32, 36, 40, 50]
    labels = ['<28', '28-32', '32-36', '36-40', 'â‰¥40']
    
    df_copy = df.copy()
    df_copy['BMI_åˆ†ç»„'] = pd.cut(df_copy['å­•å¦‡BMI'], bins=bins, labels=labels, right=False)
    
    # åˆ†æå„ç»„ç‰¹å¾
    group_analysis = {}
    
    for group in labels:
        group_data = df_copy[df_copy['BMI_åˆ†ç»„'] == group]
        
        if len(group_data) > 0:
            # åŸºæœ¬ç»Ÿè®¡
            bmi_mean = group_data['å­•å¦‡BMI'].mean()
            bmi_std = group_data['å­•å¦‡BMI'].std()
            bmi_min = group_data['å­•å¦‡BMI'].min()
            bmi_max = group_data['å­•å¦‡BMI'].max()
            
            age_mean = group_data['å¹´é¾„'].mean()
            age_std = group_data['å¹´é¾„'].std()
            
            weeks_mean = group_data['å­•å¤©'].mean() / 7
            weeks_std = group_data['å­•å¤©'].std() / 7
            weeks_median = group_data['å­•å¤©'].median() / 7
            weeks_q25 = group_data['å­•å¤©'].quantile(0.25) / 7
            weeks_q75 = group_data['å­•å¤©'].quantile(0.75) / 7
            
            y_mean = group_data['YæŸ“è‰²ä½“æµ“åº¦'].mean() * 100
            y_std = group_data['YæŸ“è‰²ä½“æµ“åº¦'].std() * 100
            y_median = group_data['YæŸ“è‰²ä½“æµ“åº¦'].median() * 100
            
            # é£é™©æŒ‡æ ‡è®¡ç®—
            timing_risk = weeks_std  # æ£€æµ‹æ—¶é—´å˜å¼‚é£é™©
            y_cv = (group_data['YæŸ“è‰²ä½“æµ“åº¦'].std() / group_data['YæŸ“è‰²ä½“æµ“åº¦'].mean())  # Yæµ“åº¦å˜å¼‚ç³»æ•°
            early_detection_rate = len(group_data[group_data['å­•å¤©'] < 84]) / len(group_data) * 100  # 12å‘¨å‰æ£€æµ‹ç‡
            sample_size_risk = 1 / (1 + len(group_data) / 50)  # æ ·æœ¬æ•°é‡é£é™©
            
            # æ¨èNIPTæ—¶ç‚¹ï¼ˆåŸºäºä¸­ä½æ•°ï¼Œè€ƒè™‘é£é™©æœ€å°åŒ–ï¼‰
            optimal_week = weeks_median
            
            # æ—¶é—´çª—å£å»ºè®®ï¼ˆåŸºäºå››åˆ†ä½æ•°ï¼‰
            time_window_lower = weeks_q25
            time_window_upper = weeks_q75
            
            analysis = {
                'count': len(group_data),
                'bmi_range': (bmi_min, bmi_max),
                'bmi_mean': bmi_mean,
                'bmi_std': bmi_std,
                'age_mean': age_mean,
                'age_std': age_std,
                'weeks_mean': weeks_mean,
                'weeks_std': weeks_std,
                'weeks_median': weeks_median,
                'weeks_q25': time_window_lower,
                'weeks_q75': time_window_upper,
                'y_mean': y_mean,
                'y_std': y_std,
                'y_median': y_median,
                'optimal_week': optimal_week,
                'time_window': (time_window_lower, time_window_upper),
                'timing_risk': timing_risk,
                'y_cv': y_cv,
                'early_detection_rate': early_detection_rate,
                'sample_size_risk': sample_size_risk,
                'composite_risk': timing_risk * 20 + y_cv * 25 + early_detection_rate * 0.3 + sample_size_risk * 15
            }
            
            group_analysis[group] = analysis
            
            sample_adequacy = 'å……è¶³' if len(group_data) >= 50 else 'ä¸€èˆ¬' if len(group_data) >= 20 else 'ä¸è¶³'
            print(f"{group}ç»„: BMI {bmi_min:.1f}-{bmi_max:.1f}, æ¨è{optimal_week:.1f}å‘¨, n={analysis['count']}({sample_adequacy}), é£é™©{analysis['composite_risk']:.1f}")
    
    return df_copy, group_analysis

boys_grouped, group_analysis = fixed_bmi_grouping_analysis(boys_data)

# æ£€æµ‹è¯¯å·®å½±å“åˆ†æ
def analyze_detection_errors(df, group_analysis, error_rates=[0.01, 0.02, 0.05, 0.10]):
    """åˆ†æä¸åŒæ£€æµ‹è¯¯å·®ç‡å¯¹å„ç»„æ£€æµ‹æˆåŠŸç‡çš„å½±å“"""
    
    error_impact = {}
    
    for error_rate in error_rates:
        group_impacts = {}
        
        for group, analysis in group_analysis.items():
            group_data = df[df['BMI_åˆ†ç»„'] == group]
            
            if len(group_data) == 0:
                continue
            
            # åŸå§‹æ£€æµ‹æˆåŠŸç‡ (Yæµ“åº¦â‰¥4%)
            original_success_rate = len(group_data[group_data['YæŸ“è‰²ä½“æµ“åº¦'] >= 0.04]) / len(group_data)
            
            # æ¨¡æ‹Ÿæ£€æµ‹è¯¯å·®
            np.random.seed(42)  # ç¡®ä¿ç»“æœå¯é‡ç°
            y_concentrations = group_data['YæŸ“è‰²ä½“æµ“åº¦'].values
            
            # æ·»åŠ æ£€æµ‹è¯¯å·®
            errors = np.random.normal(0, error_rate, len(y_concentrations))
            y_with_error = y_concentrations * (1 + errors)
            y_with_error = np.maximum(y_with_error, 0)  # ç¡®ä¿æµ“åº¦éè´Ÿ
            
            # è®¡ç®—è¯¯å·®å½±å“ä¸‹çš„æ£€æµ‹æˆåŠŸç‡
            error_success_rate = np.sum(y_with_error >= 0.04) / len(y_with_error)
            
            # è®¡ç®—å½±å“ç¨‹åº¦
            success_rate_change = (error_success_rate - original_success_rate) * 100
            relative_change = abs(success_rate_change / original_success_rate * 100) if original_success_rate > 0 else 0
            
            group_impacts[group] = {
                'original_rate': original_success_rate * 100,
                'error_rate': error_success_rate * 100,
                'absolute_change': success_rate_change,
                'relative_change': relative_change,
                'robustness': 'å¼º' if relative_change < 5 else 'ä¸­ç­‰' if relative_change < 15 else 'å¼±'
            }
        
        error_impact[error_rate] = group_impacts
    
    return error_impact

error_impact_analysis = analyze_detection_errors(boys_grouped, group_analysis)

# æœ€ç»ˆæ¨èç­–ç•¥
def generate_recommendations(group_analysis, error_impact):
    """ç”Ÿæˆæœ€ç»ˆçš„NIPTæ—¶ç‚¹æ¨èç­–ç•¥"""
    
    recommendations = {}
    
    for group, analysis in group_analysis.items():
        # ç»¼åˆè€ƒè™‘é£é™©å’Œè¯¯å·®æ•æ„Ÿæ€§
        avg_error_sensitivity = np.mean([error_impact[rate][group]['relative_change'] 
                                       for rate in error_impact.keys() if group in error_impact[rate]])
        
        # é£é™©ç­‰çº§è¯„ä¼°
        risk_level = "ä½é£é™©" if analysis['composite_risk'] < 50 else "ä¸­ç­‰é£é™©" if analysis['composite_risk'] < 80 else "é«˜é£é™©"
        
        # æ ·æœ¬å……è¶³æ€§è¯„ä¼°
        sample_adequacy = "å……è¶³" if analysis['count'] >= 50 else "ä¸€èˆ¬" if analysis['count'] >= 20 else "ä¸è¶³"
        
        recommendations[group] = {
            'bmi_range': analysis['bmi_range'],
            'optimal_week': analysis['optimal_week'],
            'time_window': analysis['time_window'],
            'sample_size': analysis['count'],
            'risk_level': risk_level,
            'error_sensitivity': avg_error_sensitivity,
            'sample_adequacy': sample_adequacy,
            'special_notes': []
        }
        
        # ç‰¹æ®Šæ³¨æ„äº‹é¡¹
        if analysis['early_detection_rate'] > 20:
            recommendations[group]['special_notes'].append("æ³¨æ„æ—©æœŸæ£€æµ‹é£é™©è¾ƒé«˜")
        if analysis['timing_risk'] > 3:
            recommendations[group]['special_notes'].append("æ£€æµ‹æ—¶é—´å˜å¼‚è¾ƒå¤§ï¼Œå»ºè®®æ›´é¢‘ç¹ç›‘æµ‹")
        if avg_error_sensitivity > 15:
            recommendations[group]['special_notes'].append("å¯¹æ£€æµ‹è¯¯å·®æ•æ„Ÿï¼Œéœ€æé«˜æ£€æµ‹ç²¾åº¦")
        if analysis['count'] < 20:
            recommendations[group]['special_notes'].append("æ ·æœ¬æ•°é‡ä¸è¶³ï¼Œå»ºè®®æ‰©å¤§æ ·æœ¬éªŒè¯")
        
        # ä¸æ‰“å°æ¯ä¸ªç»„çš„è¯¦ç»†æ¨èä¿¡æ¯
    
    return recommendations

final_recommendations = generate_recommendations(group_analysis, error_impact_analysis)

# ç”Ÿæˆåˆ†æå›¾è¡¨

# åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
fig.suptitle('åŸºäºç»éªŒçš„BMIå›ºå®šåŒºé—´åˆ†ç»„NIPTæ—¶ç‚¹ä¼˜åŒ–åˆ†æ', fontsize=16, y=0.98)

# 5.1 å„ç»„BMIåˆ†å¸ƒå’Œæ¨èæ—¶ç‚¹
ax = axes[0, 0]
groups = list(group_analysis.keys())
bmi_means = [group_analysis[g]['bmi_mean'] for g in groups]
optimal_weeks = [group_analysis[g]['optimal_week'] for g in groups]
sample_sizes = [group_analysis[g]['count'] for g in groups]

scatter = ax.scatter(bmi_means, optimal_weeks, s=[s*3 for s in sample_sizes], 
                    alpha=0.7, c=range(len(groups)), cmap='viridis')

for i, (group, bmi, week) in enumerate(zip(groups, bmi_means, optimal_weeks)):
    ax.annotate(f'{group}\n({sample_sizes[i]}äºº)', (bmi, week), 
                xytext=(5, 5), textcoords='offset points', fontsize=9)

ax.set_xlabel('å¹³å‡BMI', fontsize=12)
ax.set_ylabel('æ¨èNIPTæ—¶ç‚¹ (å‘¨)', fontsize=12)
ax.set_title('å„ç»„æ¨èNIPTæ—¶ç‚¹', fontsize=14)
ax.grid(True, alpha=0.3)

# 5.2 é£é™©è¯„åˆ†å¯¹æ¯”
ax = axes[0, 1]
risk_scores = [group_analysis[g]['composite_risk'] for g in groups]
colors = ['green' if r < 50 else 'orange' if r < 80 else 'red' for r in risk_scores]

bars = ax.bar(groups, risk_scores, color=colors, alpha=0.7)
ax.set_ylabel('ç»¼åˆé£é™©è¯„åˆ†', fontsize=12)
ax.set_title('å„ç»„é£é™©è¯„ä¼°', fontsize=14)
ax.tick_params(axis='x', rotation=45)
ax.grid(True, alpha=0.3)

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for bar, risk in zip(bars, risk_scores):
    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
            f'{risk:.1f}', ha='center', va='bottom', fontsize=10)

# 5.3 æ£€æµ‹æ—¶é—´çª—å£
ax = axes[0, 2]
time_windows_lower = [group_analysis[g]['time_window'][0] for g in groups]
time_windows_upper = [group_analysis[g]['time_window'][1] for g in groups]
time_windows_width = [upper - lower for lower, upper in zip(time_windows_lower, time_windows_upper)]

bars = ax.barh(range(len(groups)), time_windows_width, 
               left=time_windows_lower, alpha=0.7, color='lightblue')
ax.scatter(optimal_weeks, range(len(groups)), color='red', s=50, zorder=5, label='æ¨èæ—¶ç‚¹')

ax.set_yticks(range(len(groups)))
ax.set_yticklabels(groups)
ax.set_xlabel('å­•å‘¨', fontsize=12)
ax.set_title('å»ºè®®æ£€æµ‹æ—¶é—´çª—å£', fontsize=14)
ax.legend()
ax.grid(True, alpha=0.3)

# 5.4 è¯¯å·®æ•æ„Ÿæ€§åˆ†æ
ax = axes[1, 0]
error_rates = list(error_impact_analysis.keys())
for i, group in enumerate(groups):
    sensitivities = [error_impact_analysis[rate][group]['relative_change'] 
                    for rate in error_rates if group in error_impact_analysis[rate]]
    if sensitivities:
        ax.plot([r*100 for r in error_rates[:len(sensitivities)]], sensitivities, 
                'o-', label=group, alpha=0.8)

ax.set_xlabel('æ£€æµ‹è¯¯å·®ç‡ (%)', fontsize=12)
ax.set_ylabel('ç›¸å¯¹å˜åŒ– (%)', fontsize=12)
ax.set_title('è¯¯å·®æ•æ„Ÿæ€§åˆ†æ', fontsize=14)
ax.legend()
ax.grid(True, alpha=0.3)

# 5.5 æ ·æœ¬åˆ†å¸ƒ
ax = axes[1, 1]
sample_counts = [group_analysis[g]['count'] for g in groups]
colors = ['green' if c >= 50 else 'orange' if c >= 20 else 'red' for c in sample_counts]

bars = ax.bar(groups, sample_counts, color=colors, alpha=0.7)
ax.set_ylabel('æ ·æœ¬æ•°é‡', fontsize=12)
ax.set_title('å„ç»„æ ·æœ¬åˆ†å¸ƒ', fontsize=14)
ax.tick_params(axis='x', rotation=45)
ax.axhline(y=50, color='green', linestyle='--', alpha=0.7, label='å……è¶³æ ·æœ¬çº¿')
ax.axhline(y=20, color='orange', linestyle='--', alpha=0.7, label='æœ€ä½æ ·æœ¬çº¿')
ax.legend()
ax.grid(True, alpha=0.3)

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for bar, count in zip(bars, sample_counts):
    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, 
            f'{count}', ha='center', va='bottom', fontsize=10)

# 5.6 ç»¼åˆæ¨èæ€»ç»“
ax = axes[1, 2]
ax.axis('off')

summary_text = "æœ€ç»ˆæ¨èç­–ç•¥æ€»ç»“\n\n"
for group, rec in final_recommendations.items():
    risk_color = "ğŸŸ¢" if rec['risk_level'] == "ä½é£é™©" else "ğŸŸ¡" if rec['risk_level'] == "ä¸­ç­‰é£é™©" else "ğŸ”´"
    summary_text += f"{risk_color} {group}: {rec['optimal_week']:.1f}å‘¨\n"
    summary_text += f"   BMI {rec['bmi_range'][0]:.1f}-{rec['bmi_range'][1]:.1f}\n"
    summary_text += f"   çª—å£ {rec['time_window'][0]:.1f}-{rec['time_window'][1]:.1f}å‘¨\n\n"

summary_text += "\né£é™©ç­‰çº§:\nğŸŸ¢ ä½é£é™©  ğŸŸ¡ ä¸­ç­‰é£é™©  ğŸ”´ é«˜é£é™©"

ax.text(0.05, 0.95, summary_text, transform=ax.transAxes, fontsize=11,
        verticalalignment='top', bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray", alpha=0.8))

plt.tight_layout()
plt.show()

# ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
total_samples = sum([analysis['count'] for analysis in group_analysis.values()])
avg_risk = np.mean([analysis['composite_risk'] for analysis in group_analysis.values()])

print(f"\næ¨èç­–ç•¥:")
for group, rec in final_recommendations.items():
    print(f"  {group}ç»„: BMI {rec['bmi_range'][0]:.1f}-{rec['bmi_range'][1]:.1f}, æ¨è{rec['optimal_week']:.1f}å‘¨, n={rec['sample_size']}({rec['sample_adequacy']}), é£é™©{rec['risk_level']}")

print(f"æ€»ä½“: {total_samples}äºº, å¹³å‡é£é™©{avg_risk:.1f}, {len(group_analysis)}ç»„")

# ä¿å­˜ç»“æœ
boys_grouped.to_csv('boys_normal_analysis.csv', index=False, encoding='utf-8-sig')
print(f"å·²ä¿å­˜: boys_normal_analysis.csv")